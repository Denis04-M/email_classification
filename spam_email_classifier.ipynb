{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "spam_email_classifier.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP3rhVu59L921ltetMODIqF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Denis04-M/email_classification/blob/main/spam_email_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "dvNscwEKuRoG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f962d201-e02a-4646-fcbd-f457c15c1dc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "filepath1 = '/content/drive/MyDrive/data/enron1/ham/'\n",
        "filepath2 = '/content/drive/MyDrive/data/enron1/spam/'\n",
        "\n",
        "emails, labels = [], []\n",
        "\n",
        "# appending valid emails into emails[]\n",
        "for filename in glob.glob(os.path.join(filepath1, '*.txt')):\n",
        "  with open(filename, 'r', encoding='ISO-8859-1') as email_file:\n",
        "    emails.append(email_file.read())\n",
        "    labels.append(1) # labeling each valid email as 1\n",
        "\n",
        "# appending spam emails into emils[]\n",
        "for filename in glob.glob(os.path.join(filepath2, '*.txt')):\n",
        "  with open(filename, 'r', encoding='ISO-8859-1') as email_file:\n",
        "    emails.append(email_file.read())\n",
        "    labels.append(0) # labeling each spam email as 0\n",
        "\n",
        "print(len(emails))\n",
        "print(len(labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7VwEuty7jIL",
        "outputId": "c2addfbc-a463-4b65-dd2c-921f1c142f3d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5172\n",
            "5172\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import names\n",
        "from  nltk.stem import WordNetLemmatizer\n",
        "\n",
        "people_names = set(names.words())\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "# function to remove numbers and special characters\n",
        "def letters_only(astr):\n",
        "  return astr.isalpha()\n",
        "\n",
        "# cleaning the data (removin)\n",
        "def cleanText(docs):\n",
        "  cleaned_docs = []\n",
        "  for doc in docs:\n",
        "    cleaned_docs.append(\" \".join([lemmatizer.lemmatize(word.lower())\n",
        "      for word in doc.split()\n",
        "        if letters_only(word) and word not in people_names]))\n",
        "  return cleaned_docs\n",
        "\n",
        "cleaned_emails = cleanText(emails)\n",
        "\n",
        "print(cleaned_emails[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwZgY8mhCo_Z",
        "outputId": "e75c130d-9ad3-4759-fe2f-eb46714f6925"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tried to get fancy with your address and it came back to me forwarded by lauri a allen hol aepin on pm to daren farmer enron com cc subject daren your rate for meter highlander central point for year starting delivered to equistar channelview is mm that price expires in week on november let me know if you need me to refresh after that time thanks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from collections import defaultdict\n",
        "\n",
        "cv = CountVectorizer(stop_words = 'english', max_features=500)\n",
        "\n",
        "term_docs = cv.fit_transform(cleaned_emails)\n",
        "\n",
        "def get_label_index(labels):\n",
        "  labeld_index = defaultdict(list)\n",
        "  for index, label in enumerate(labels):\n",
        "    label_index[label].append(index)\n",
        "  return label_index\n",
        "\n",
        "label_index = get_label_index(labels)\n",
        "\n",
        "def get_prior(label_index):\n",
        "  prior = {label: len(index) for label, index in label_index.items()}\n",
        "  total_count = sum(prior.values())\n",
        "  for label in prior:\n",
        "    prior[label] /= float(total_count)\n",
        "  return prior\n",
        "\n",
        "prior = get_prior(label_index)\n",
        "print(prior)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQ5GTYABBzuZ",
        "outputId": "11df1b73-87d6-404b-dda6-18721c2abc16"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{1: 0.7099767981438515, 0: 0.2900232018561485}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_likelihood(term_document_matrix, label_index, smoothing = 0):\n",
        "  likelihood = {}\n",
        "  for label, index in label_index.items():\n",
        "    likelihood[label] = term_document_matrix[index, :].sum(axis=0) + smoothing\n",
        "    likelihood[label] = np.asarray(likelihood[label])[0]\n",
        "    total_count = likelihood[label].sum()\n",
        "    likelihood[label] = likelihood[label] / float(total_count)\n",
        "  return likelihood\n",
        "\n",
        "smoothing = 1\n",
        "likelihood = get_likelihood(term_docs, label_index, smoothing)\n",
        "\n",
        "print(likelihood[0][:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7b8XsSobLwL",
        "outputId": "5eaa0a41-4603-49ce-dbee-3229e08ec18a"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.00108188 0.00141386 0.00458837 0.00052167 0.00423564]\n"
          ]
        }
      ]
    }
  ]
}